Cet article est le premier d'une serie ayant pour but de decrire et formaliser les differentes methodes et tricks utilises dans mes 3 demos pour js1k 2013. Les principaux themes abordes seront donc la generation procedural d'elements 3D, leur rendering, et comment faire rentrer tout ca sur 1024 bytes.

For today, we will deal with "how to easily render my 3D scene in a 2D canvas", presenting one of the simplest method for that: the Painter's Algorithm.

Rendering process

You have at hand your various 3D meshes, nicely placed in a scene, and now you wonder how to render that into a static picture or a dynamic demo... Le procede general est relativement simple:
- Definir la camera (position, orientation, field of view,...)
- Projeter vos 3D meshes sur son viewfinder 2D plan
- Reporter et dessiner les elements visibles sur votre canvas

----------------
OPEN YOUR THIRD EYE

So first you have to decide what you want to see, and how. Where will be positioned your camera? Which direction will it be aiming? How far/near can it see? With which ratio? With a realistic perspective projection, an orthographic one or something more exotic? ...
There are many parameters with barbaric names (try to place "frustum" in a conversation...) to take into account. We won't address all of them right now, so for now, let's only deal with:
	- A perspective projection (the furthest, the smallest on screen).
	- A camera positioned at (Cx, Cy, Cz) in the cartesian space, with its orientation defined as (yaw, pitch, roll), the Tait–Bryan angles.
	- The screen ratio, while respecting the proportions in our scene. We call W the screen's width and H its height.

----------------
FLATTEN THE WORLD

The projection step is quite straightforward. 

[ Expliquer les maths derriere ]

[Matrix Formula]
This matrix formula looks good, but can be laborious to use if you can't afford to include a library dealing with matrix operations. So let's linearize it. We thus get (thanks Wikipedia for saving me from this burden):
[Linear Formula]

Nothing we can't implement here. Moreover, you can observe some recurrences in it, which means it is possible to refactore it to save some operations and bytes:
[Refactored Formula]

We now can compute the positions of 3D elements with respect to the coordinate system defined by our camera (with X and Y are the projection position on the viewfinder and Z the perspective/depth seen from the camera). But it is not over yet: our final medium is the screen/canvas we want to display our scene on, not the camera's viewfinder. We thus have to apply a last transform to get the pixels positions:
[Expliquer projection dans image]
	The division by Dz is the key for the perspective effect (the furthest from the camera, the bigger Dz, and thus the smallest the element).
	Through the multiplication of Dx and Dy by min(H,W), we apply the chosen ratio while preserving the proportions (try to use max(H,W) or to use W with Dx and H with Dy to see how it influences the result).
	Finally, the addition of half the screen's dimensions centers the elements.

So here is our final projection function, to be applied to every vertex of our scene:
[Function // expliquer que le clipping est fait par le canvas]

There is one important thing I haven't addressed yet : the "clipping" step. When you look at something with your own eyes or a camera, you don't see the "whole scene", e.g. you can't see behind you. Your field of view, your "frustum", is limited to a specific cone or pyramid (truncated, if you take into account the near plane and the far plane. Some cameras can't see objects too close or too far: the planes represent those limits).
In the above method, we project every element, without disgarding the ones out of view. We need to get rid of them, to "clip" them.
In my examples, to do that, I simply rely on a property of the HTML canvas: its limits, and its pliability when it comes to　draw out of them. With our transforms, pixels of elements out of the frustum will have their position (Bx, By) out of the canvas field, and when I blindly ask to draw them, the canvas API doesn't complain: the results will just not appear.
It is far from an efficient solution, and easy improvements can be done (for instance to check by yourself the values of Bx and By before drawing). The best solution would be to clip the scene before the projection, in the 3D space, but here things get a bit more tricky.
Finally, if you want to truncate your field of view to a certain depth range (delimited by the near and far planes), you can simply disgard the projections with a Dz out of it.

----------------
PAINTER'S STROKE

Objects beyond our perepheral vision are not the only on which are hidden. Some can be totally or partially covered by others.
So we don't want to draw our elements higgledy-piggledy, displaying faces which should be hidden under others. We want to preserve the depth order while rendering. And that's exactly what the Painter's algorithm is for.
The idea is quite simple: before displaying our elements, we just sort them by decreasing depth (so from the furthest from the camera position to the closest). By drawing them in this order, the front elements will naturally cover/overlay the ones behind. Like most painters would do.
Luckily for us, we've already computed the depth of the vertices, Dz, making the implementation of the rendering function really straightforward:
[Function]

And that's all. We just implemented a really simple 3D renderer (which can be quite useful, for instance if you are dealing with 3D objects in a project, and you need to display them only for testing, and thus don't want to deal with new dependencies just for that... Well, it did happen to me).

But don't be fooled: when I say "simple renderer", I especially mean "too simple". The Painter's algorithm has many drawbacks, such as:
- It draws stuff which finally won't be displayed (hidden under). Far from optimal... (unless you have semi-transparent objects in your scene: this flaw becomes a cheap way to automatically manage the transparency)
- It is "face-wise", ie. it draws face per face, and not pixel per pixel like some other methods. We thus lose in precision (the pixel-wise solutions give you the possibility to compute textures more precisely - pixel per pixel), and we can also face some rendering glitches, for instance in the case of overlapping or piercing polygons (how to order thoses faces?)
- As implemented here, we don't know which element is above/under which one. This information could be useful to apply some effects, such as shadows (see for example the [http://web.cs.wpi.edu/~matt/courses/cs563/talks/shadow/shadow.html]Shadow Z-buffer algorithm).

That being said, this method can still be applied to many cases

=====================================================
Morphose - Explanations

Morphose is the first demo I posted in js1k 2013.

The main idea is the following: I generate two meshes, a cube and a sphere, with the same number of vertices/faces (each one is associated with an other one of the 2nd mesh). Then every frame, I compute an intermediate mesh from a pseudo-tweening between the geometries of the two original ones, and display it. Add some colors and interactions, and voila!
Now let's dig into the details of each step:

-------------
Mesh generation

	First we'll deal with the creation of our two meshes, the cube and the sphere, with as few bytes as possible. A mesh is a complex structure, usually defined by faces and vertices, with many implementations (for instance, in my article about the Painter's Algorithm, I've opted for Mesh: Array<Faces> with Faces: Array<Vertex> and Vertex:Array<float>. Thanks to the underlying use of pointers, this implementation wat quite light).
	For my demo, I chose a different solution. Each face is directly an array of float, like this:
		TriangularFace = [X1, Y1, Z1, X2, Y2, Z2, X3, Y3, Z3]
	This structure is more compact, but heavier: every face has its vertices values hard-coded. Unlike the previous implementation where the faces shared pointers to the same common vertices, here each face is independant to the other ones (the vertices are duplicated). So we aren't exactly manipulating a coherent mesh, but a collection of faces we could explode at will (which I do in the demo).
	That being fixed, we can now define our objects:

-----	
Cube
	The objective is to define a cube of size D, centered/oriented on the origin (to simplify the generation), and made of 6*N^2 triangular faces (N^2 per side).
	So... how can we define our cube? Here is a first possibility:
	- C = {S0, S1, S2, S3, S4, S5} with:
		- Side S0 : { (x, y, z) e S0 if x and y e [-D/2, D/2] and z = +D/2 }
		- Side S1 : { (x, y, z) e S1 if y and z e [-D/2, D/2] and z = -D/2 }
		- Side S2 : { (x, y, z) e S2 if z and x e [-D/2, D/2] and y = +D/2 }
		- Side S3 : { (x, y, z) e S3 if x and y e [-D/2, D/2] and z = -D/2 }
		- Side S4 : { (x, y, z) e S4 if y and z e [-D/2, D/2] and x = +D/2 }
		- Side S5 : { (x, y, z) e S5 if z and x e [-D/2, D/2] and z = -D/2 }
		
	Do you see the pattern? Here is a condensed definition:
	- C = {Sn, n e [0,5]} with:
		- Side Sn : { v = (x , y, z), (v[n%3], v[(n+1)%3], v[(n+2)%3)]) e Sn if x and y e [-D/2, D/2] and z = D/2 - D*(n%2) }
	
	We use a modulo 3 operation to express the cyclic permutation of the coordinates (the two varrying and the one fixed), and a modulo 2 operation to define the value of the fixed coordinate. With this, we cover the 6 cases...

	The only thing left is to split the sides into triangular faces (if you want to use quadfaces, the modifications are minor), which can be easily achieved with loops to sample a regular repartition of vertices. The final generation method is:
	[Code]

-----
Sphere
	Now we want a sphere of diameter D, made of the same number 6*N^2 of faces.
	There are many possible ways to tesselate a sphere, but we have one condition we wish to fulfill: to get a smooth transition when tweening the geometries of our cube and sphere. So we want a *similar* repartition of the vertices.
	
	[Exemple with tweening between cube and ECP sphere]
	
	The quadcube tesselation (or cubic sphere tesselation) is what we want for our sphere. This method is commonly used to map flat textures to spheres, but can also be adapted to generate their mesh.
	The idea is to take a regular mesh of a cube (so like the one we generated), and to project every vertex on the spherical surface, by normalizing the length of the vector to the center of the mesh, to the value we want as radius.
	In other word, for every vertex, we compute their distance to the center, divide their coordinates by the obtained value, and multiplie them back with the radius value.
	
	[2D example]
	
	Thus, by just adding these two operations to our previous tesselation function, we can return a sphere instead of a cube (or both):
	
	[Code]
	
	[Demo showing tweening between cube and sphere]
	
-------------
Rendering
	We have our two 3D objects, and thus crave to first display them. For that, we can use the simple renderer described in my previous article. Only minor modifications are required, to adapt to the specificity of our mesh structure:
	
	[Code]
	
	
